{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOc36/YD4kORZvhpxQGjym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raida03/AI360s2fall2024/blob/main/Lab07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzO6X0Yifn35",
        "outputId": "057d7e47-0868-48ed-dcac-4a8f83ad511f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.4.tar.gz (7.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/7.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/7.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (11.0.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.20.1+cu121)\n",
            "Collecting ttach (from grad-cam)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
            "Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.4-py3-none-any.whl size=39648 sha256=a41230dc7dfb22691b34cde5c8f5185475d41f92adcb73e8e3972c554577dced\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/b0/82/1f97b5348c7fe9f0ce0ba18497202cafa5dec4562bd5292680\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.5.4 ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "9HMOPShJgRvV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "dt2deWekgimm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XV1L6LAjUyT",
        "outputId": "9a384856-0195-4df7-f5c4-970b3921a922"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/caltech-101.zip\"\n",
        "extract_path = \"/content/drive/MyDrive/caltech-101\"  # Directory to extract to\n",
        "\n",
        "with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n"
      ],
      "metadata": {
        "id": "ZWsfV8E6kSY7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(os.path.join(extract_path, '__MACOSX'), ignore_errors=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "wo4gv7xXlUfy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for root, dirs, files in os.walk(extract_path):\n",
        "    print(f\"Root: {root}, Files: {files}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMu502J6lwPC",
        "outputId": "41a7db72-319e-4060-9826-b380593fd0c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root: /content/drive/MyDrive/caltech-101, Files: []\n",
            "Root: /content/drive/MyDrive/caltech-101/caltech-101, Files: ['101_ObjectCategories.tar.gz', 'show_annotation.m', 'Annotations.tar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
      ],
      "metadata": {
        "id": "46by77f8t-o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n"
      ],
      "metadata": {
        "id": "KunqaD6buC0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "VKf3uVPJuGuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg19 = models.vgg19(pretrained=True)\n",
        "vgg19.classifier[6] = nn.Linear(4096, 101)\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "resnet50.fc = nn.Linear(2048, 101)\n",
        "\n",
        "efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n",
        "efficientnet_b0.classifier[1] = nn.Linear(1280, 101)"
      ],
      "metadata": {
        "id": "9ZatORkWuNej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_vgg = optim.Adam(vgg19.parameters(), lr=0.001)\n",
        "optimizer_resnet = optim.Adam(resnet50.parameters(), lr=0.001)\n",
        "optimizer_efficientnet = optim.Adam(efficientnet_b0.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hbyvlv0GuRVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            y_pred.extend(preds.numpy())\n",
        "            y_true.extend(labels.numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "53ogherGuZ5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating VGG19...\")\n",
        "evaluate_model(vgg19, test_loader)\n",
        "\n",
        "print(\"Evaluating ResNet50...\")\n",
        "evaluate_model(resnet50, test_loader)\n",
        "\n",
        "print(\"Evaluating EfficientNet-B0...\")\n",
        "evaluate_model(efficientnet_b0, test_loader)\n",
        "\n",
        "print(\"Evaluation completed!\")"
      ],
      "metadata": {
        "id": "oFzqDkp_ueQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_grad_cam(model, target_layer, test_loader):\n",
        "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images_np = images[0].permute(1, 2, 0).numpy()\n",
        "        grayscale_cam = cam(input_tensor=images.unsqueeze(0), target_category=labels[0].item())\n",
        "        cam_image = show_cam_on_image(images_np, grayscale_cam[0], use_rgb=True)\n",
        "\n",
        "        plt.imshow(cam_image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        break"
      ],
      "metadata": {
        "id": "YdaSME5fujCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying Grad-CAM on VGG19...\")\n",
        "apply_grad_cam(vgg19, vgg19.features[-1], test_loader)\n",
        "\n",
        "# Apply Grad-CAM on ResNet50\n",
        "print(\"Applying Grad-CAM on ResNet50...\")\n",
        "apply_grad_cam(resnet50, resnet50.layer4[-1], test_loader)\n",
        "\n",
        "# Apply Grad-CAM on EfficientNet-B0\n",
        "print(\"Applying Grad-CAM on EfficientNet-B0...\")\n",
        "apply_grad_cam(efficientnet_b0, efficientnet_b0.features[-1], test_loader)\n",
        "\n",
        "print(\"Grad-CAM visualization completed!\")\n"
      ],
      "metadata": {
        "id": "qyPpD1tFun1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}